{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the following corpus, modified from the one in the chapter: <br><br>\n",
    "&lt;s&gt; I am Sam &lt;/s&gt; <br>\n",
    "&lt;s&gt; Sam I am &lt;/s&gt; <br>\n",
    "&lt;s&gt; I am Sam &lt;/s&gt; <br>\n",
    "&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt; <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1: <br>\n",
    "Using a <b> bigram </b> language model with add-one smoothing, <br>\n",
    "what is <b> P(Sam | am)? </b> <br> \n",
    "Include &lt;s&gt; and &lt;/s&gt; in your counts just like any other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is: 11\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<s>\", \"</s>\", \"I\", \"am\", \"Sam\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\"]\n",
    "print(f\"The length of the vocabulary is: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>       : 4\n",
      "I         : 4\n",
      "am        : 3\n",
      "Sam       : 4\n",
      "</s>      : 4\n",
      "do        : 1\n",
      "not       : 1\n",
      "like      : 1\n",
      "green     : 1\n",
      "eggs      : 1\n",
      "and       : 1\n",
      "---------------\n",
      "Corpus Size: 25\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "          \"<s>\", 'I', 'am', 'Sam', \"</s>\", \n",
    "          \"<s>\",'Sam', 'I', 'am', \"</s>\",\n",
    "          \"<s>\", 'I', 'am', 'Sam', \"</s>\",\n",
    "          \"<s>\",'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'Sam', \"</s>\",\n",
    "          ]\n",
    "\n",
    "token_frequency = {}\n",
    "\n",
    "# Find Unique Tokens and Count occurences\n",
    "for token in corpus:\n",
    "    if token not in token_frequency:\n",
    "        token_frequency[token] = 1\n",
    "    else: \n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# Display Tokens\n",
    "for token in token_frequency:\n",
    "    print(f\"{token.ljust(10)}: {token_frequency[token]}\")\n",
    "\n",
    "# Count Total number of tokens\n",
    "corpus_size = 0 \n",
    "for token in token_frequency:\n",
    "    corpus_size += token_frequency[token]\n",
    "\n",
    "print(\"---------------\")\n",
    "print(f\"Corpus Size: {corpus_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding <b> P(Sam | am) </b> <br>\n",
    "Formula to find bigram applying laplace smoothing / +1 smoothing, <br> <br>\n",
    "\n",
    "$$\n",
    "P(Sam | am) = P(W_{i-1}, W_i | W_{i-1}) = \\frac{C(W_{i-1}, W_i) + 1}{C(W_{i-1}) + V}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\usflo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Corpus is already tokenized based off of input, so we can just find bigrams\n",
    "bigram_list = list(bigrams(corpus))\n",
    "\n",
    "# Count Occurences of each bigram using Counter object\n",
    "bigram_counts = Counter(bigram_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'I')         : 3\n",
      "('I', 'am')          : 3\n",
      "('am', 'Sam')        : 2\n",
      "('Sam', '</s>')      : 3\n",
      "('</s>', '<s>')      : 3\n",
      "('<s>', 'Sam')       : 1\n",
      "('Sam', 'I')         : 1\n",
      "('am', '</s>')       : 1\n",
      "('I', 'do')          : 1\n",
      "('do', 'not')        : 1\n",
      "('not', 'like')      : 1\n",
      "('like', 'green')    : 1\n",
      "('green', 'eggs')    : 1\n",
      "('eggs', 'and')      : 1\n",
      "('and', 'Sam')       : 1\n"
     ]
    }
   ],
   "source": [
    "for bigram in bigram_counts:\n",
    "    print(f\"{str(bigram).ljust(20)} : {bigram_counts[bigram]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Sam | Am) == P(am Sam) / P (am) == C (count_am_sam_bigram + 1) / C(count_am + V) = 0.10714285714285714\n"
     ]
    }
   ],
   "source": [
    "count_am_sam_bigram = bigram_counts[('am', 'Sam')]\n",
    "count_am = token_frequency['am']\n",
    "\n",
    "print(f\"P(Sam | Am) == P(am Sam) / P (am) == C (count_am_sam_bigram + 1) / C(count_am + V) = { (count_am_sam_bigram + 1) / (count_am + corpus_size) }\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions!\n",
    "1. In the corpus we see multiple start and end token, do we account for them multiple times or just once? \n",
    "2. In my bigram counts, do i count keep accounting for start and end token, because at one point I got a bigram of (start token, end token)? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

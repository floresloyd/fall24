{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 1 PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Load Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Under the trust 's Fit for the Future proposals , two of the hospitals would lose their A&E units with consultant - led maternity services also possibly being centralised on one site .\n",
      "2: Man charged over drugs seizure\n",
      "3: Borrowing from your 401 ( k ) or withdrawing from your IRA , on the other hand , can make sense in some situations , says Michael Chasnoff , a financial planner in Cincinnati : \" If going back to school helps you command a higher level of income , there is a return on your investment . \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Train and Test\n",
    "\n",
    "# Load the train.txt file\n",
    "with open('train.txt', 'r', encoding='utf-8') as train_file:\n",
    "    train_sentences = [line.strip() for line in train_file.readlines()]\n",
    "\n",
    "# Load the test.txt file\n",
    "with open('test.txt', 'r', encoding='utf-8') as test_file:\n",
    "    test_sentences = [line.strip() for line in test_file.readlines()]\n",
    "\n",
    "# Test if we loaded sentences properly\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}: {train_sentences[i]}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "# for i in range(len(test_sentences)):\n",
    "#     print(f\"{i+1}: {test_sentences[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad each sentence in the training and test corpora with start and end symbols (you can use &lt;s&gt; and &lt;/s&gt;, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <s> Under the trust 's Fit for the Future proposals , two of the hospitals would lose their A&E units with consultant - led maternity services also possibly being centralised on one site . </s>\n",
      "2: <s> Man charged over drugs seizure </s>\n",
      "3: <s> Borrowing from your 401 ( k ) or withdrawing from your IRA , on the other hand , can make sense in some situations , says Michael Chasnoff , a financial planner in Cincinnati : \" If going back to school helps you command a higher level of income , there is a return on your investment . \" </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "padded_train_sentences = train_sentences\n",
    "padded_test_sentences = test_sentences\n",
    "\n",
    "# Iterate over each sentence using a range based for loop as strings are immutable, hence we have to overwrite indeces\n",
    "for i in range(len(padded_train_sentences)):\n",
    "    padded_train_sentences[i] = \"<s> \" + padded_train_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(padded_test_sentences)):\n",
    "    padded_test_sentences[i] = \"<s> \" + padded_test_sentences[i] + \" </s>\"\n",
    "\n",
    "\n",
    "# Test if we padded our sentences properly\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}: {padded_train_sentences[i]}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(f\"{i+1}: {padded_test_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase all words in the training and test corpora. Note that the data already has been tokenized (i.e. the punctuation has been split off words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <s> under the trust 's fit for the future proposals , two of the hospitals would lose their a&e units with consultant - led maternity services also possibly being centralised on one site . </s>\n",
      "2: <s> man charged over drugs seizure </s>\n",
      "3: <s> borrowing from your 401 ( k ) or withdrawing from your ira , on the other hand , can make sense in some situations , says michael chasnoff , a financial planner in cincinnati : \" if going back to school helps you command a higher level of income , there is a return on your investment . \" </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowered_padded_train_sentences = train_sentences\n",
    "lowered_padded_test_sentences = test_sentences\n",
    "\n",
    "for i in range(len(lowered_padded_train_sentences)):\n",
    "    lowered_padded_train_sentences[i] = padded_train_sentences[i].lower() \n",
    "\n",
    "# for i in range(len(lowered_padded_test_sentences)):\n",
    "#     lowered_padded_test_sentences[i] = lowered_padded_test_sentences[i].lower() \n",
    "\n",
    "# Test if we lowered sentences properly\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}: {lowered_padded_train_sentences[i]}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "# for i in range(len(test_sentences)):\n",
    "#     print(f\"Lowered Test Sentence {i+1}: {lowered_padded_test_sentences[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all words occurring in the training data once with the token &lt;unk&gt; . Every word in the test data not seen in training should be treated as &lt;unk&gt;.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Obtained ...\n"
     ]
    }
   ],
   "source": [
    "# Obtain Vocabulary | List of unique tokens  \n",
    "\n",
    "vocabulary = set()      # Using a set reduces the complexity into 0(n + m) rather than 0(n^2) as there is no need to check for membership of the token since sets handle duplicates\n",
    "\n",
    "for sentence in lowered_padded_train_sentences:\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        vocabulary.add(token)\n",
    "\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary Obtained ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 83045\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies counted ...\n"
     ]
    }
   ],
   "source": [
    "# Counting frequency of all tokens \n",
    "frequency_count = {}\n",
    "\n",
    "for sentence in lowered_padded_train_sentences:\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in frequency_count:\n",
    "            frequency_count[token] += 1\n",
    "        else:\n",
    "            frequency_count[token] = 1\n",
    "\n",
    "# for token in vocabulary:\n",
    "#     print(f\"{str(token).ljust(20)} : {frequency_count[token]}\")\n",
    "\n",
    "print(\"Frequencies counted ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 2568210 tokens in the corpus.\n"
     ]
    }
   ],
   "source": [
    "corpus_size = 0\n",
    "\n",
    "for token in frequency_count:\n",
    "    corpus_size += frequency_count[token]\n",
    "\n",
    "print(f\"There are a total of {corpus_size} tokens in the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that only appeared once accounted for. There are 41307 that appeared only once.\n"
     ]
    }
   ],
   "source": [
    "# Identify words that occured only once and change value to the uknown token\n",
    "tokens_that_appeared_only_once = []\n",
    "\n",
    "for token in frequency_count:\n",
    "    if frequency_count[token] <= 1:\n",
    "        tokens_that_appeared_only_once.append(token)\n",
    "\n",
    "#print(tokens_that_appeared_only_once)\n",
    "number_of_tokens_that_appeared_only_once = len(tokens_that_appeared_only_once)\n",
    "print(f\"Tokens that only appeared once accounted for. There are {number_of_tokens_that_appeared_only_once} that appeared only once.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens that appeared less than once to &lt;unk&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that appear only once have been replaced with <unk>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences once before the loop\n",
    "tokenized_sentences = [sentence.split() for sentence in lowered_padded_train_sentences]\n",
    "\n",
    "# Initialize an empty list for processed sentences\n",
    "processed_train_sentences = []\n",
    "\n",
    "# Loop over each tokenized sentence\n",
    "for sentence in tokenized_sentences:\n",
    "    # Create a list to hold the processed tokens\n",
    "    processed_sentence = []\n",
    "    \n",
    "    # Loop over each token in the sentence\n",
    "    for token in sentence:\n",
    "        # Replace token with <unk> if it appears only once\n",
    "        if token in tokens_that_appeared_only_once:\n",
    "            processed_sentence.append(\"<unk>\")\n",
    "        else:\n",
    "            processed_sentence.append(token)\n",
    "    \n",
    "    # Join the processed tokens and append to the final list\n",
    "    processed_train_sentences.append(\" \".join(processed_sentence))\n",
    "\n",
    "print(\"Tokens that appear only once have been replaced with <unk>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens that appeared only once  : 41307\n",
      "Total number of <unk> tokens in the corpus: 41307\n",
      "Are they equal? : True\n"
     ]
    }
   ],
   "source": [
    "# Verify that you replaced all the tokens that appearaed only once with the unkown token.\n",
    "\n",
    "unk_count = 0\n",
    "\n",
    "for sentence in processed_train_sentences:\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    for token in sentence:\n",
    "\n",
    "        if token == \"<unk>\":\n",
    "            unk_count += 1\n",
    "\n",
    "\n",
    "print(f\"Number of tokens that appeared only once  : {number_of_tokens_that_appeared_only_once}\")\n",
    "print(f\"Total number of <unk> tokens in the corpus: {unk_count}\")\n",
    "\n",
    "print(f\"Are they equal? : {number_of_tokens_that_appeared_only_once == unk_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.2 Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence frequencies counted ...\n"
     ]
    }
   ],
   "source": [
    "# Counting frequency of processed tokens\n",
    "processed_frequency_count = {}\n",
    "\n",
    "for sentence in processed_train_sentences:\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in processed_frequency_count:\n",
    "            processed_frequency_count[token] += 1\n",
    "        else:\n",
    "            processed_frequency_count[token] = 1\n",
    "\n",
    "\n",
    "print(\"Train sentence frequencies counted ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41739 \n",
      "Corpus Size: 2568210\n"
     ]
    }
   ],
   "source": [
    "n_processed_train = len(processed_frequency_count)\n",
    "n_corpus_size = 0\n",
    "\n",
    "for token in processed_frequency_count:\n",
    "    n_corpus_size += processed_frequency_count[token]\n",
    "\n",
    "print(f\"Vocabulary size: {n_processed_train} \\nCorpus Size: {n_corpus_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIGRAM MLE\n",
    "P(w) = \n",
    "\n",
    "$$\n",
    "\\frac{\\text{count of\\ w}}{\\text{total number of words in the corpus}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 0.05\n",
      ", : 0.04\n",
      "<s> : 0.04\n",
      "</s> : 0.04\n",
      ". : 0.03\n",
      "to : 0.02\n",
      "of : 0.02\n",
      "and : 0.02\n",
      "a : 0.02\n",
      "in : 0.02\n"
     ]
    }
   ],
   "source": [
    "unigram_mle_probabilities = {}\n",
    "\n",
    "for token in processed_frequency_count:\n",
    "    unigram_mle_probabilities[token] =  processed_frequency_count[token] / n_corpus_size\n",
    "\n",
    "\n",
    "# Sort the unigram probabilities in descending order by probability values\n",
    "top_10_unigrams = sorted(unigram_mle_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 unigrams with their probabilities, formatted to 2 decimal places\n",
    "for token, prob in top_10_unigrams:\n",
    "    print(f\"{token} : {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGRAM MLE\n",
    "$$\n",
    "P(w_2 \\mid w_1) = \\frac{\\text{count of } (w_1, w_2)}{\\text{count of } w_1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams counted. Highest bigram frequencies: \n",
      "\n",
      "('.', '</s>') : 82888\n",
      "('<s>', 'the') : 12233\n",
      "('of', 'the') : 11832\n",
      "('in', 'the') : 10640\n",
      "(',', '\"') : 9598\n",
      "(',', 'the') : 6998\n",
      "('<unk>', ',') : 6222\n",
      "('<s>', '\"') : 6123\n",
      "(',', 'and') : 6083\n",
      "('to', 'the') : 4757\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = {}\n",
    "\n",
    "for sentence in processed_train_sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    \n",
    "    # Iterate over sentence with index to count bigrams correctly\n",
    "    for i in range(len(words_in_sentence) - 1): # Iterate through word array only once\n",
    "        w1, w2 = words_in_sentence[i], words_in_sentence[i + 1]\n",
    "        \n",
    "        if (w1, w2) not in bigram_counts:\n",
    "            bigram_counts[(w1, w2)] = 1\n",
    "        else: \n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "\n",
    "print(\"Bigrams counted. Highest bigram frequencies: \\n\")\n",
    "# Sort the bigrams by their counts in descending order\n",
    "sorted_bigrams = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Use a for loop to print the top 10 bigrams\n",
    "for i in range(10):\n",
    "    bigram, count = sorted_bigrams[i]\n",
    "    print(f\"{bigram} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams probabilities computed. Highest bigram probability: \n",
      "\n",
      "('ã\"â', '...') : 1.0\n",
      "('18-year', '-') : 1.0\n",
      "('852', '-') : 1.0\n",
      "('2520', '-') : 1.0\n",
      "('esophagus', ',') : 1.0\n",
      "('conjunction', 'with') : 1.0\n",
      "('psychosomatic', 'medicine') : 1.0\n",
      "('experimented', 'with') : 1.0\n",
      "('yellen', 'said') : 1.0\n",
      "('swaths', 'of') : 1.0\n"
     ]
    }
   ],
   "source": [
    "bigram_mle_probabilities = {}\n",
    "\n",
    "for bigram in bigram_counts:\n",
    "    bigram_mle_probabilities[bigram] = bigram_counts[bigram] / processed_frequency_count[bigram[0]]\n",
    "\n",
    "\n",
    "print(\"Bigrams probabilities computed. Highest bigram probability: \\n\")\n",
    "# Sort the bigrams by their counts in descending order\n",
    "sorted_bigram_probability = sorted(bigram_mle_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Use a for loop to print the top 10 bigrams\n",
    "for i in range(10):\n",
    "    bigram, count = sorted_bigram_probability[i]\n",
    "    print(f\"{bigram} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGRAM MLE + 1 Smoothing\n",
    "$$\n",
    "P(w_2 \\mid w_1) = \\frac{\\text{count of } (w_1, w_2) + 1}{\\text{count of } w_1 + V}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', '</s>') : 0.03121\n",
      "('<s>', 'the') : 0.00459\n",
      "('of', 'the') : 0.00452\n",
      "('in', 'the') : 0.00408\n",
      "(',', '\"') : 0.00358\n",
      "(',', 'the') : 0.00261\n",
      "('<unk>', ',') : 0.00238\n",
      "('<s>', '\"') : 0.00230\n",
      "(',', 'and') : 0.00227\n",
      "('to', 'the') : 0.00182\n"
     ]
    }
   ],
   "source": [
    "# Bigram +1 smoothing\n",
    "bigram_mle_plus1_probabilities = {}\n",
    "\n",
    "# Calculate smoothed bigram probabilities\n",
    "for bigram in bigram_counts:\n",
    "    bigram_mle_plus1_probabilities[bigram] = (bigram_counts[bigram] + 1) / (processed_frequency_count[bigram[0]] + n_corpus_size)\n",
    "\n",
    "# Sort the bigrams by their smoothed probabilities in descending order\n",
    "sorted_bigram_plus1_probability = sorted(bigram_mle_plus1_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Use a for loop to print the top 10 bigrams with smoothed probabilities\n",
    "for i in range(10):\n",
    "    bigram, prob = sorted_bigram_plus1_probability[i]\n",
    "    print(f\"{bigram} : {prob:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

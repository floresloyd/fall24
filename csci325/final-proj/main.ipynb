{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the project \n",
    "- The goal of your project is to develop a supervised machine learning algorithm (deep\n",
    "or not) that is able to predict presence/absence of at least 5 classes in a test image.\n",
    "- animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "- PASCAL Visual Object Classes (VOC) challenge\n",
    "(http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ ) has been used heavily by the\n",
    "computer vision community for the developments of machine (deep) learning\n",
    "algorisms in classification, detection and segmentation. There are 20 classes in total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "1. `Annotations`: Contains XML files with bounding box annotations for objects in each image.\n",
    "2. `ImageSets`: Contains lists of image IDs for different splits (e.g., train, val, trainval).\n",
    "3. `JPEGImages`: Contains the actual image files.\n",
    "4. `SegmentationClass`: Contains semantic segmentation masks for images.\n",
    "5. `SegmentationObject` : Contains instance segmentation masks for images.\n",
    "\n",
    "<br>\n",
    "VOCdevkit/ <br>\n",
    "    ├── VOC2012/ <br>\n",
    "    │   ├── Annotations/    # XML files for bounding boxes <br>\n",
    "    │   ├── ImageSets/      # Splits: train.txt, val.txt <br>\n",
    "    │   ├── JPEGImages/     # Raw images <br>\n",
    "    │   ├── SegmentationClass/     # Raw images <br>\n",
    "    │   ├── SegmentationObject/     # Raw images <br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Set up environment by importing libraries \n",
    "- $ `pip install numpy pandas torch torchvision matplotlib opencv-python xmltodict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os                                            # working with file systems\n",
    "import xml.etree.ElementTree as ET                   # Used for parsing XML files (Annotations)\n",
    "import cv2                                           # Computer Vision Library used to manipulate (resize, convert format, normalize pixel values) images.\n",
    "import numpy as np                                   # Numerical computations for multi-dimensional arrays\n",
    "import torch                                         # deep learning library \n",
    "import torch.nn as nn                                # contains modules to build neural netwroks\n",
    "import torch.optim as optim                          # Provides optimization algorithms for training neural networks.\n",
    "from sklearn.metrics import classification_report    # machine learning library, we specifically need metrics\n",
    "from torch.utils.data import DataLoader, TensorDataset #  used for handling datasets and data loading\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 -  Extract Information \n",
    "- Extract relevant data (image paths and labels) from the XML annotation files. We do this by importing a custom method called `parse_annotations` from `methods.py`\n",
    "- I am only interested in the animals of the dataset which is why i add the animal_classes to filter out non-animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths extracted ...\n",
      "train data extracted here are the first 5 results ...\n",
      "[('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000008.jpg', {'horse'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000019.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000053.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000060.jpg', {'cat'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000066.jpg', {'dog'})]\n",
      "val data extracted here are the first 5 results ...\n",
      "[('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000009.jpg', {'cow'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000026.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000054.jpg', {'bird'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000056.jpg', {'cat'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000059.jpg', {'dog'})]\n"
     ]
    }
   ],
   "source": [
    "from methods import parse_annotations  # custom method to parse annotations\n",
    "from methods import parse_animal_annotations\n",
    "\n",
    "annotations_directory = \"./data/VOCdevkit/VOC2012/Annotations\"\n",
    "image_directory = \"./data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "train_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/train.txt\"\n",
    "val_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/val.txt\"\n",
    "print(\"paths extracted ...\")\n",
    "\n",
    "\n",
    "# Define your target classes\n",
    "animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n",
    "\n",
    "# extract annotations for the training set\n",
    "train_data = parse_animal_annotations(annotations_directory, image_directory, train_file, animal_classes)\n",
    "val_data = parse_animal_annotations(annotations_directory, image_directory, val_file, animal_classes)\n",
    "\n",
    "# View the first 5 results\n",
    "print(\"train data extracted here are the first 5 results ...\")\n",
    "print(train_data[:5])\n",
    "print(\"val data extracted here are the first 5 results ...\")\n",
    "print(val_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted classes: {'cat', 'sheep', 'dog', 'cow', 'bird', 'horse'}\n"
     ]
    }
   ],
   "source": [
    "all_labels = {label for _, labels in train_data for label in labels}\n",
    "print(f\"Extracted classes: {all_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Preprocess the Data\n",
    "- `Resize image` to a fixed `224x224` size to ensure uniformity. This is also the standard size for pretrained models and deep learning models as they require uniformity.\n",
    "- `Normalize pixel` values to 0, 1 for easier processing. This is done by utilizing the custom preprocess_images method from `methods.py`\n",
    "- `Convert labels to multi-hot vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 2091 images.\n",
      "Preprocessed 2108 images.\n"
     ]
    }
   ],
   "source": [
    "from methods import preprocess_images\n",
    "\n",
    "train_images = preprocess_images(train_data, size=(224, 224))\n",
    "print(f\"Preprocessed {len(train_images)} images.\")\n",
    "\n",
    "val_images = preprocess_images(val_data, size=(224, 224))\n",
    "print(f\"Preprocessed {len(val_images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Convert labels into Multi-hot vectors so the machine understands it\n",
    "- 1 -> appears\n",
    "- 0 -> did not appear\n",
    "- animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label vectors shape: (2091, 6)\n",
      "val Label vectors shape: (2108, 6)\n"
     ]
    }
   ],
   "source": [
    "from methods import convert_labels\n",
    "\n",
    "\n",
    "# Convert training labels to multi-hot vectors\n",
    "train_labels = convert_labels(train_data, animal_classes)\n",
    "val_labels = convert_labels(val_data, animal_classes)\n",
    "print(f\"Train Label vectors shape: {train_labels.shape}\")\n",
    "print(f\"val Label vectors shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer instantiated ...\n"
     ]
    }
   ],
   "source": [
    "from AnimalClassifier import AnimalClassifier\n",
    "\n",
    "num_classes = len(animal_classes)  # 6 total animals hence 6 classes\n",
    "model = AnimalClassifier(num_classes)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_function = nn.BCELoss()  # binary cross entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # adam optimizer \n",
    "\n",
    "print(\"Model, loss function, and optimizer instantiated ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).permute(0, 3, 1, 2)  \n",
    "val_images_tensor = torch.tensor(val_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(train_images_tensor, train_labels_tensor), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_images_tensor, val_labels_tensor), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4696575514687837\n",
      "Epoch 2/5, Loss: 0.41325606235111034\n",
      "Epoch 3/5, Loss: 0.39668550882630677\n",
      "Epoch 4/5, Loss: 0.3669066847735689\n",
      "Epoch 5/5, Loss: 0.30983609507102094\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 82.04%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Evaluate model with kfold cross validation to get real results\n",
    "- The results from the previous model are unrealiable as it will definitely get a perfect score as it is being evaluated on things it has seen to get more results we must run k-fold cross validation to ensure reliable results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters and Files loaded ...\n",
      "Data Parsed ...\n",
      "Data Merged ...\n",
      "Labels and images preprocessed ...\n",
      "Converted to tesnors ...\n",
      "Dataset combined ...\n",
      "Kfold instantiated ...\n",
      "Running kfold ...\n",
      "Fold 1/5\n",
      "Fold 1, Epoch 1/10, Loss: 0.4306306763773873\n",
      "Fold 1, Epoch 2/10, Loss: 0.37848365278471086\n",
      "Fold 1, Epoch 3/10, Loss: 0.3372248018781344\n",
      "Fold 1, Epoch 4/10, Loss: 0.2428471116083009\n",
      "Fold 1, Epoch 5/10, Loss: 0.11148268358693236\n",
      "Fold 1, Epoch 6/10, Loss: 0.0381811100723488\n",
      "Fold 1, Epoch 7/10, Loss: 0.012470793176908046\n",
      "Fold 1, Epoch 8/10, Loss: 0.005253448743314948\n",
      "Fold 1, Epoch 9/10, Loss: 0.0016444652394663232\n",
      "Fold 1, Epoch 10/10, Loss: 0.0004755425120189708\n",
      "Fold 1, Validation Accuracy: 81.17%\n",
      "Fold 2/5\n",
      "Fold 2, Epoch 1/10, Loss: 0.4510710601295744\n",
      "Fold 2, Epoch 2/10, Loss: 0.4188549921626136\n",
      "Fold 2, Epoch 3/10, Loss: 0.4024598536037263\n",
      "Fold 2, Epoch 4/10, Loss: 0.38725172465755825\n",
      "Fold 2, Epoch 5/10, Loss: 0.36543412804603576\n",
      "Fold 2, Epoch 6/10, Loss: 0.3363013320025944\n",
      "Fold 2, Epoch 7/10, Loss: 0.2630682341399647\n",
      "Fold 2, Epoch 8/10, Loss: 0.15190954123224532\n",
      "Fold 2, Epoch 9/10, Loss: 0.0648282438915755\n",
      "Fold 2, Epoch 10/10, Loss: 0.028327369173279122\n",
      "Fold 2, Validation Accuracy: 81.01%\n",
      "Fold 3/5\n",
      "Fold 3, Epoch 1/10, Loss: 0.4144080878723235\n",
      "Fold 3, Epoch 2/10, Loss: 0.37778056604521615\n",
      "Fold 3, Epoch 3/10, Loss: 0.32426408451227917\n",
      "Fold 3, Epoch 4/10, Loss: 0.23331432129655566\n",
      "Fold 3, Epoch 5/10, Loss: 0.12335388351763998\n",
      "Fold 3, Epoch 6/10, Loss: 0.04937007912612032\n",
      "Fold 3, Epoch 7/10, Loss: 0.016229326745849988\n",
      "Fold 3, Epoch 8/10, Loss: 0.00489382584485048\n",
      "Fold 3, Epoch 9/10, Loss: 0.0015516119991107623\n",
      "Fold 3, Epoch 10/10, Loss: 0.0004920754490220653\n",
      "Fold 3, Validation Accuracy: 80.28%\n",
      "Fold 4/5\n",
      "Fold 4, Epoch 1/10, Loss: 0.43193237157095044\n",
      "Fold 4, Epoch 2/10, Loss: 0.397542359999248\n",
      "Fold 4, Epoch 3/10, Loss: 0.3724738918599628\n",
      "Fold 4, Epoch 4/10, Loss: 0.3027977011033467\n",
      "Fold 4, Epoch 5/10, Loss: 0.1751726485079243\n",
      "Fold 4, Epoch 6/10, Loss: 0.06914359676163821\n",
      "Fold 4, Epoch 7/10, Loss: 0.028008331012512957\n",
      "Fold 4, Epoch 8/10, Loss: 0.012601698820002465\n",
      "Fold 4, Epoch 9/10, Loss: 0.011189803392127422\n",
      "Fold 4, Epoch 10/10, Loss: 0.007157630698341\n",
      "Fold 4, Validation Accuracy: 80.97%\n",
      "Fold 5/5\n",
      "Fold 5, Epoch 1/10, Loss: 0.4306318393775395\n",
      "Fold 5, Epoch 2/10, Loss: 0.3907860612585431\n",
      "Fold 5, Epoch 3/10, Loss: 0.35298071588788715\n",
      "Fold 5, Epoch 4/10, Loss: 0.27164072536286854\n",
      "Fold 5, Epoch 5/10, Loss: 0.13271091578616984\n",
      "Fold 5, Epoch 6/10, Loss: 0.049462699047511535\n",
      "Fold 5, Epoch 7/10, Loss: 0.01602336999161967\n",
      "Fold 5, Epoch 8/10, Loss: 0.007432296919259464\n",
      "Fold 5, Epoch 9/10, Loss: 0.0027420143165814113\n",
      "Fold 5, Epoch 10/10, Loss: 0.0011363588543775091\n",
      "Fold 5, Validation Accuracy: 81.96%\n"
     ]
    }
   ],
   "source": [
    "# Everything done above in one code snippet\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from AnimalClassifier import AnimalClassifier\n",
    "from methods import parse_animal_annotations, preprocess_images, convert_labels\n",
    "\n",
    "\n",
    "annotation_dir = \"./data/VOCdevkit/VOC2012/Annotations\"\n",
    "image_dir = \"./data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "train_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/train.txt\"\n",
    "val_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/val.txt\"\n",
    "target_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n",
    "image_size = (224, 224)\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "k_folds = 5\n",
    "print(\"Parameters and Files loaded ...\")\n",
    "\n",
    "# Parse and preprocess train and validation data\n",
    "train_data = parse_animal_annotations(annotation_dir, image_dir, train_file, target_classes)\n",
    "val_data = parse_animal_annotations(annotation_dir, image_dir, val_file, target_classes)\n",
    "print(\"Data Parsed ...\")\n",
    "\n",
    "# Merge train and val data\n",
    "merged_data = train_data + val_data\n",
    "print(\"Data Merged ...\")\n",
    "\n",
    "# Preprocess images and labels\n",
    "images = preprocess_images(merged_data, size=image_size)\n",
    "class_list = target_classes\n",
    "labels = convert_labels(merged_data, class_list)\n",
    "print(\"Labels and images preprocessed ...\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)  # Channels first\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "print(\"Converted to tesnors ...\")\n",
    "\n",
    "# Combine into a single dataset\n",
    "dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "print(\"Dataset combined ...\")\n",
    "\n",
    "# Initialize K-Fold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "print(\"Kfold instantiated ...\")\n",
    "\n",
    "print(\"Running kfold ...\")\n",
    "# Initialize the model, loss function, and optimizer\n",
    "num_classes = len(target_classes)\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "    # Split data into train and validation subsets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "    # Model, criterion, and optimizer\n",
    "    model = AnimalClassifier(num_classes)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "    print(f\"Fold {fold+1}, Validation Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

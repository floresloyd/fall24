{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the project \n",
    "- The goal of your project is to develop a supervised machine learning algorithm (deep\n",
    "or not) that is able to predict presence/absence of at least 5 classes in a test image.\n",
    "- animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "- PASCAL Visual Object Classes (VOC) challenge\n",
    "(http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ ) has been used heavily by the\n",
    "computer vision community for the developments of machine (deep) learning\n",
    "algorisms in classification, detection and segmentation. There are 20 classes in total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "1. `Annotations`: Contains XML files with bounding box annotations for objects in each image.\n",
    "2. `ImageSets`: Contains lists of image IDs for different splits (e.g., train, val, trainval).\n",
    "3. `JPEGImages`: Contains the actual image files.\n",
    "4. `SegmentationClass`: Contains semantic segmentation masks for images.\n",
    "5. `SegmentationObject` : Contains instance segmentation masks for images.\n",
    "\n",
    "<br>\n",
    "VOCdevkit/ <br>\n",
    "    ├── VOC2012/ <br>\n",
    "    │   ├── Annotations/    # XML files for bounding boxes <br>\n",
    "    │   ├── ImageSets/      # Splits: train.txt, val.txt <br>\n",
    "    │   ├── JPEGImages/     # Raw images <br>\n",
    "    │   ├── SegmentationClass/     # Raw images <br>\n",
    "    │   ├── SegmentationObject/     # Raw images <br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Set up environment by importing libraries \n",
    "- $ `pip install numpy pandas torch torchvision matplotlib opencv-python xmltodict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os                                            # working with file systems\n",
    "import xml.etree.ElementTree as ET                   # Used for parsing XML files (Annotations)\n",
    "import cv2                                           # Computer Vision Library used to manipulate (resize, convert format, normalize pixel values) images.\n",
    "import numpy as np                                   # Numerical computations for multi-dimensional arrays\n",
    "import torch                                         # deep learning library \n",
    "import torch.nn as nn                                # contains modules to build neural netwroks\n",
    "import torch.optim as optim                          # Provides optimization algorithms for training neural networks.\n",
    "from sklearn.metrics import classification_report    # machine learning library, we specifically need metrics\n",
    "from torch.utils.data import DataLoader, TensorDataset #  used for handling datasets and data loading\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 -  Extract Information \n",
    "- Extract relevant data (image paths and labels) from the XML annotation files. We do this by importing a custom method called `parse_annotations` from `methods.py`\n",
    "- I am only interested in the animals of the dataset which is why i add the animal_classes to filter out non-animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths extracted ...\n",
      "train data extracted here are the first 5 results ...\n",
      "[('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000008.jpg', {'horse'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000019.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000053.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000060.jpg', {'cat'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000066.jpg', {'dog'})]\n",
      "val data extracted here are the first 5 results ...\n",
      "[('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000009.jpg', {'cow'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000026.jpg', {'dog'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000054.jpg', {'bird'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000056.jpg', {'cat'}), ('./data/VOCdevkit/VOC2012/JPEGImages\\\\2008_000059.jpg', {'dog'})]\n"
     ]
    }
   ],
   "source": [
    "from methods import parse_annotations  # custom method to parse annotations\n",
    "from methods import parse_animal_annotations\n",
    "\n",
    "annotations_directory = \"./data/VOCdevkit/VOC2012/Annotations\"\n",
    "image_directory = \"./data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "train_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/train.txt\"\n",
    "val_file = \"./data/VOCdevkit/VOC2012/ImageSets/Main/val.txt\"\n",
    "print(\"paths extracted ...\")\n",
    "\n",
    "\n",
    "# Define your target classes\n",
    "animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n",
    "\n",
    "# extract annotations for the training set\n",
    "train_data = parse_animal_annotations(annotations_directory, image_directory, train_file, animal_classes)\n",
    "val_data = parse_animal_annotations(annotations_directory, image_directory, val_file, animal_classes)\n",
    "\n",
    "# View the first 5 results\n",
    "print(\"train data extracted here are the first 5 results ...\")\n",
    "print(train_data[:5])\n",
    "print(\"val data extracted here are the first 5 results ...\")\n",
    "print(val_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted classes: {'cat', 'sheep', 'dog', 'cow', 'bird', 'horse'}\n"
     ]
    }
   ],
   "source": [
    "all_labels = {label for _, labels in train_data for label in labels}\n",
    "print(f\"Extracted classes: {all_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Preprocess the Data\n",
    "- `Resize image` to a fixed `224x224` size to ensure uniformity. This is also the standard size for pretrained models and deep learning models as they require uniformity.\n",
    "- `Normalize pixel` values to 0, 1 for easier processing. This is done by utilizing the custom preprocess_images method from `methods.py`\n",
    "- `Convert labels to multi-hot vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 2091 images.\n",
      "Preprocessed 2108 images.\n"
     ]
    }
   ],
   "source": [
    "from methods import preprocess_images\n",
    "\n",
    "train_images = preprocess_images(train_data, size=(224, 224))\n",
    "print(f\"Preprocessed {len(train_images)} images.\")\n",
    "\n",
    "val_images = preprocess_images(val_data, size=(224, 224))\n",
    "print(f\"Preprocessed {len(val_images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Convert labels into Multi-hot vectors so the machine understands it\n",
    "- 1 -> appears\n",
    "- 0 -> did not appear\n",
    "- animal_classes = [\"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label vectors shape: (2091, 6)\n",
      "val Label vectors shape: (2108, 6)\n"
     ]
    }
   ],
   "source": [
    "from methods import convert_labels\n",
    "\n",
    "\n",
    "# Convert training labels to multi-hot vectors\n",
    "train_labels = convert_labels(train_data, animal_classes)\n",
    "val_labels = convert_labels(val_data, animal_classes)\n",
    "print(f\"Train Label vectors shape: {train_labels.shape}\")\n",
    "print(f\"val Label vectors shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer instantiated ...\n"
     ]
    }
   ],
   "source": [
    "from AnimalClassifier import AnimalClassifier\n",
    "\n",
    "num_classes = len(animal_classes)  # 6 total animals hence 6 classes\n",
    "model = AnimalClassifier(num_classes)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_function = nn.BCELoss()  # binary cross entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # adam optimizer \n",
    "\n",
    "print(\"Model, loss function, and optimizer instantiated ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).permute(0, 3, 1, 2)  \n",
    "val_images_tensor = torch.tensor(val_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(train_images_tensor, train_labels_tensor), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_images_tensor, val_labels_tensor), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4696575514687837\n",
      "Epoch 2/5, Loss: 0.41325606235111034\n",
      "Epoch 3/5, Loss: 0.39668550882630677\n",
      "Epoch 4/5, Loss: 0.3669066847735689\n",
      "Epoch 5/5, Loss: 0.30983609507102094\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 82.04%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Evaluate model with kfold cross validation to get real results\n",
    "- The results from the previous model are unrealiable as it will definitely get a perfect score as it is being evaluated on things it has seen to get more results we must run k-fold cross validation to ensure reliable results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
